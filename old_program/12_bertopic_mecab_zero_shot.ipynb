{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d338c13-f463-4696-b64e-be50b9fbc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from mecab import MeCab\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from bertopic.representation import KeyBERTInspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cb7711-0476-4278-9614-027aa679d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file :umsun_A_2_sborder.csv ...\n",
      "Loading file :umsun_B_sborder.csv ...\n",
      "Loading file :umsun_clean_C.csv ...\n"
     ]
    }
   ],
   "source": [
    "IN_DATA_PATH = './data/01_out'\n",
    "OUT_DATA_PATH = './data/02_out'\n",
    "\n",
    "in_data_files = []\n",
    "in_docs = []\n",
    "total_docs = []\n",
    "\n",
    "# 폴더내 파일리스트를 가져온다.\n",
    "input_files = os.listdir(IN_DATA_PATH)\n",
    "\n",
    "# 입력 파일 로딩 RAW_DATA_PATH에 있는 파일을 DataFrame으로 읽어 드린다.\n",
    "for in_file in input_files:\n",
    "    print (\"Loading file :%s ...\"%in_file)\n",
    "    df = pd.read_csv(os.path.join(IN_DATA_PATH,in_file), encoding='utf-8-sig')\n",
    "    docs = df['generated_text'].tolist()\n",
    "    in_data_files.append(df)\n",
    "    in_docs.append(docs)\n",
    "    total_docs.extend(docs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8522611e-f8c8-4adc-bae9-7c650990a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "        word_tokens = self.tagger.morphs(sent)\n",
    "        result = [word for word in word_tokens]\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e066afca-521e-4721-8557-571374f0adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer = CustomTokenizer(MeCab())\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afa4ea7-faf8-4441-8ba4-8b479204fee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6726e41c5a94a4b846e40eebb34a25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-calculate embeddings\n",
    "\n",
    "#embedding_model = SentenceTransformer(\"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "embedding_model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "embeddings = embedding_model.encode(total_docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a408628-53be-4ffe-b1fb-542cc2da669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원축소\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19c1f70-1ddb-4b19-b9ab-bb9ebfca447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot\n",
    "ko_classifier = pipeline(\n",
    "    task='zero-shot-classification',\n",
    "    model='MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli',\n",
    "    device=0,\n",
    "    # hypothesis_template='구매하는 이유는 {} 이다.',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e5c485-4aec-4073-97a1-ed359d3ad3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_LABELS = ['첨가물','성분','위험','아기','안심','칼로리','중위험','아이']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8e15e64-20bc-4596-8fa4-c3c63c1d6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] => A파일 , [1] => B파일 , [2] => C파일\n",
    "\n",
    "docs = in_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d44e2a9-399b-4f1c-bdd4-543b448bb5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>첨가물</td>\n",
       "      <td>[첨가물이, 첨가물, 첨가물은, 첨가물도, 맛있는데, 맛있어요, 우유에, 먹어요, ...</td>\n",
       "      <td>[첨가물이 좀 있네요., 첨가물이 있네요., 첨가물이 없어서 좋아요.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>성분</td>\n",
       "      <td>[성분, 성분이, 성분은, 성분들이, 첨가물도, 음료에, 성분도, 알레르기라, 마시...</td>\n",
       "      <td>[저위험 성분이 있네요., 중위험 성분이 있어서 아쉬워요., 중위험 성분이 들어가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>안심</td>\n",
       "      <td>[무난해요, 무난합니다, 무난, 무난한, 괜찮네요, 괜찮았어요, 괜찮더라고요, 만족...</td>\n",
       "      <td>[무난해요., 무난해요., 가격 대비 괜찮아요.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>아기</td>\n",
       "      <td>[아기가, 아기, 아기도, 아기들, 먹였어요, 아가가, 먹어요, 아가, 아가들은, ...</td>\n",
       "      <td>[첫 요구르트로 사줘 봤는데 요구르트 당 함량은 어쩔 수 없나 봐요 근데 아기가 안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>칼로리</td>\n",
       "      <td>[칼로리, 칼로리가, 먹어요, 다이어트할, 함량이, 간식으로, 먹으려고요, 맛있긴,...</td>\n",
       "      <td>[당류가 낮아서 다이어트할 때 간식으로 많이 먹어요., 넘나 맛있는 골드 키위 맛 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>중위험</td>\n",
       "      <td>[중위험이, 중위험이네, 중위험이라서, 중위험, 사리면, 아쉽다, 있었군요, 있네요...</td>\n",
       "      <td>[중위험이 아쉽다., 중위험이 있었군요., 중위험이 있네요.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>아이</td>\n",
       "      <td>[아이, 아이보다, 아이가, 착하네요, 제품입니다, 아이에게, 좋아해요, 추천합니다...</td>\n",
       "      <td>[아이 배냇 추천합니다., 아이가 정말 좋아하는 제품입니다 제품이 착하네요., 아이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>위험</td>\n",
       "      <td>[저위험은, , , , , , , , , ]</td>\n",
       "      <td>[저위험은 없네요.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>-1_카레_먹어요_먹었어요_먹어야겠어요</td>\n",
       "      <td>[카레, 먹어요, 먹었어요, 먹어야겠어요, 맛있는, 카레를, 맛이에요, 먹으면, 카...</td>\n",
       "      <td>[카레는 백세 카레 사다 먹어요., 이렇게 많은 갖가지 재료 가루들을 넣고도 맛없기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1064</td>\n",
       "      <td>0_맛있어요_먹어요_좋네요_맛있는</td>\n",
       "      <td>[맛있어요, 먹어요, 좋네요, 맛있는, 좋아해요, 맛도, 좋아요, 있어요, 좋아해서...</td>\n",
       "      <td>[이거 맛있고 양도 많아서 자주 먹는데 위험 성분이 없어서 안심하고 먹을 수 있어서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>1_맛있어요_맛있네요_맛있습니다_맛있어</td>\n",
       "      <td>[맛있어요, 맛있네요, 맛있습니다, 맛있어, 맛나요, 맛도, 괜찮더라고요, 맛나, ...</td>\n",
       "      <td>[맛있어요., 맛있어요., 맛있어요.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>2_만두예요_만두인데_만두네요_만두는</td>\n",
       "      <td>[만두예요, 만두인데, 만두네요, 만두는, 만두에, 만두라기보단, 만두가, 만두를,...</td>\n",
       "      <td>[고향만두는 추억의 만두 같아요 어렸을 적부터 먹던 만두., 사이즈가 만두 사이즈가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>3_피자네요_피자라서_피자가_피자로</td>\n",
       "      <td>[피자네요, 피자라서, 피자가, 피자로, 피자를, 피자, 핫소스, 피자점에서, 피자...</td>\n",
       "      <td>[오뚜기 콤비네이션 피자 가격도 저렴하고 맛도 좋아서 또 생각이 날 정도로 맛있어요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>4_고래밥이_고래밥_고래밥을_고래밥이네요</td>\n",
       "      <td>[고래밥이, 고래밥, 고래밥을, 고래밥이네요, 고래상어, 고래, 맛있다고는, 먹었지...</td>\n",
       "      <td>[아스파탐 뇌 종양 이하 아스파탐 왜 넣는 거니까 아스파탐이 아마도 당류 성분인 거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>5_새우깡보다는_새우깡도_새우깡보다_새우에</td>\n",
       "      <td>[새우깡보다는, 새우깡도, 새우깡보다, 새우에, 새우깡을, 새우깡, 새우의, 새우를...</td>\n",
       "      <td>[자극 없이 맘 놓고 먹일 수 있는 상품이죠 어른인 저희 부부고 시중의 새우보다도 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                     Name  \\\n",
       "0       0    176                      첨가물   \n",
       "1       1    109                       성분   \n",
       "2       2     40                       안심   \n",
       "3       3     28                       아기   \n",
       "4       4      8                      칼로리   \n",
       "5       5      8                      중위험   \n",
       "6       6      6                       아이   \n",
       "7       7      1                       위험   \n",
       "8       8     97    -1_카레_먹어요_먹었어요_먹어야겠어요   \n",
       "9       9   1064       0_맛있어요_먹어요_좋네요_맛있는   \n",
       "10     10     31    1_맛있어요_맛있네요_맛있습니다_맛있어   \n",
       "11     11     25     2_만두예요_만두인데_만두네요_만두는   \n",
       "12     12     24      3_피자네요_피자라서_피자가_피자로   \n",
       "13     13     22   4_고래밥이_고래밥_고래밥을_고래밥이네요   \n",
       "14     14     21  5_새우깡보다는_새우깡도_새우깡보다_새우에   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [첨가물이, 첨가물, 첨가물은, 첨가물도, 맛있는데, 맛있어요, 우유에, 먹어요, ...   \n",
       "1   [성분, 성분이, 성분은, 성분들이, 첨가물도, 음료에, 성분도, 알레르기라, 마시...   \n",
       "2   [무난해요, 무난합니다, 무난, 무난한, 괜찮네요, 괜찮았어요, 괜찮더라고요, 만족...   \n",
       "3   [아기가, 아기, 아기도, 아기들, 먹였어요, 아가가, 먹어요, 아가, 아가들은, ...   \n",
       "4   [칼로리, 칼로리가, 먹어요, 다이어트할, 함량이, 간식으로, 먹으려고요, 맛있긴,...   \n",
       "5   [중위험이, 중위험이네, 중위험이라서, 중위험, 사리면, 아쉽다, 있었군요, 있네요...   \n",
       "6   [아이, 아이보다, 아이가, 착하네요, 제품입니다, 아이에게, 좋아해요, 추천합니다...   \n",
       "7                            [저위험은, , , , , , , , , ]   \n",
       "8   [카레, 먹어요, 먹었어요, 먹어야겠어요, 맛있는, 카레를, 맛이에요, 먹으면, 카...   \n",
       "9   [맛있어요, 먹어요, 좋네요, 맛있는, 좋아해요, 맛도, 좋아요, 있어요, 좋아해서...   \n",
       "10  [맛있어요, 맛있네요, 맛있습니다, 맛있어, 맛나요, 맛도, 괜찮더라고요, 맛나, ...   \n",
       "11  [만두예요, 만두인데, 만두네요, 만두는, 만두에, 만두라기보단, 만두가, 만두를,...   \n",
       "12  [피자네요, 피자라서, 피자가, 피자로, 피자를, 피자, 핫소스, 피자점에서, 피자...   \n",
       "13  [고래밥이, 고래밥, 고래밥을, 고래밥이네요, 고래상어, 고래, 맛있다고는, 먹었지...   \n",
       "14  [새우깡보다는, 새우깡도, 새우깡보다, 새우에, 새우깡을, 새우깡, 새우의, 새우를...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0             [첨가물이 좀 있네요., 첨가물이 있네요., 첨가물이 없어서 좋아요.]  \n",
       "1   [저위험 성분이 있네요., 중위험 성분이 있어서 아쉬워요., 중위험 성분이 들어가 ...  \n",
       "2                         [무난해요., 무난해요., 가격 대비 괜찮아요.]  \n",
       "3   [첫 요구르트로 사줘 봤는데 요구르트 당 함량은 어쩔 수 없나 봐요 근데 아기가 안...  \n",
       "4   [당류가 낮아서 다이어트할 때 간식으로 많이 먹어요., 넘나 맛있는 골드 키위 맛 ...  \n",
       "5                  [중위험이 아쉽다., 중위험이 있었군요., 중위험이 있네요.]  \n",
       "6   [아이 배냇 추천합니다., 아이가 정말 좋아하는 제품입니다 제품이 착하네요., 아이...  \n",
       "7                                         [저위험은 없네요.]  \n",
       "8   [카레는 백세 카레 사다 먹어요., 이렇게 많은 갖가지 재료 가루들을 넣고도 맛없기...  \n",
       "9   [이거 맛있고 양도 많아서 자주 먹는데 위험 성분이 없어서 안심하고 먹을 수 있어서...  \n",
       "10                              [맛있어요., 맛있어요., 맛있어요.]  \n",
       "11  [고향만두는 추억의 만두 같아요 어렸을 적부터 먹던 만두., 사이즈가 만두 사이즈가...  \n",
       "12  [오뚜기 콤비네이션 피자 가격도 저렴하고 맛도 좋아서 또 생각이 날 정도로 맛있어요...  \n",
       "13  [아스파탐 뇌 종양 이하 아스파탐 왜 넣는 거니까 아스파탐이 아마도 당류 성분인 거...  \n",
       "14  [자극 없이 맘 놓고 먹일 수 있는 상품이죠 어른인 저희 부부고 시중의 새우보다도 ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer,\n",
    "        nr_topics=\"auto\", # 문서를 대표하는 토픽의 갯수\n",
    "        # top_n_words=4,\n",
    "        zeroshot_topic_list=CANDIDATE_LABELS,\n",
    "        zeroshot_min_similarity=.5,\n",
    "        # representation_model=ko_classifier,\n",
    "        representation_model=KeyBERTInspired(),\n",
    "        calculate_probabilities=True\n",
    ")\n",
    "\t\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "topic_model.get_topic_info()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a84c933c-4fc5-4dc0-b751-244ebb242007",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hierarchical_topics \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhierarchical_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tree \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mget_topic_tree(hierarchical_topics)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/bertopic/_bertopic.py:975\u001b[0m, in \u001b[0;36mBERTopic.hierarchical_topics\u001b[0;34m(self, docs, linkage_function, distance_function)\u001b[0m\n\u001b[1;32m    972\u001b[0m     linkage_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: sch\u001b[38;5;241m.\u001b[39mlinkage(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m'\u001b[39m, optimal_ordering\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# Calculate distance\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_tf_idf_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outliers\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    976\u001b[0m X \u001b[38;5;241m=\u001b[39m distance_function(embeddings)\n\u001b[1;32m    977\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_distance_matrix(X, embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6505fd8f-1e6b-4d64-af6a-6beebe5d4504",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mbuild_analyzer()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract features for topic coherence evaluation\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [analyzer(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m cleaned_docs]\n\u001b[1;32m     17\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(tokens)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1485\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_names_out\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m        Transformed feature names.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m   1487\u001b[0m         [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[1;32m   1488\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[1;32m   1489\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:508\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_:\n\u001b[0;32m--> 508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary not fitted or provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# Coherence\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Preprocess documents for coherence evaluation\n",
    "documents = pd.DataFrame({\"Document\": docs, \"ID\": range(len(docs)), \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ''.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer for BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for topic coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# Extract words in each topic if they are non-empty and exist in the dictionary\n",
    "topic_words = []\n",
    "#topic_words = [list(zip(*model_2024_B_3.get_topic(topic)))[0] for topic in range(len(set(topics))-1)]\n",
    "for topic in range(len(set(topics))-topic_model._outliers):\n",
    "  words = list(zip(*topic_model.get_topic(topic)))[0]\n",
    "  words = [word for word in words if word in dictionary.token2id]\n",
    "  topic_words.append(words)\n",
    "topic_words = [words for words in topic_words if len(words)>0]\n",
    "\n",
    "# Evaluate Coherence\n",
    "coherence_model = CoherenceModel(topics=topic_words, texts=tokens, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(\"Coherence Score: \", coherence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
